{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the data\n",
    "# Take a Quick Look at the Data Structure\n",
    "- Head\n",
    "- Info\n",
    "- Describe\n",
    "- Histogram on numerical features\n",
    "# Create a Test Set\n",
    "housing_train, housing_test = train_test_split(housing_data, test_size=0.2)\n",
    "housing_train.shape, housing_test.shape\n",
    "# Discover and Visualize the Data to Gain Insights\n",
    "## Visualizing Geographical Data\n",
    "## Looking for Correlations\n",
    "## Experimenting with Feature Engineering\n",
    "housing_train_df = housing_train_df.assign(rooms_per_household=housing_train_df[\"total_rooms\"]/housing_train_df[\"households\"])\n",
    "\n",
    "# Data Preparation\n",
    "## Variable Encoding\n",
    "# il n'y a qu'une variable de type catégoriel, on peut donc utiliser la méthode du One-hot encoding pour obtenir des variables numériques\n",
    "housing_train_df = pd.get_dummies(housing_train_df)\n",
    "\n",
    "## Data Cleaning (missing values handling)\n",
    "# Check a class called Imputer in Scikit Learn with parameter \"median\"\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "    #imputation par la moyenne\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "housing_train_df_imp = imp.fit_transform(housing_train_df)\n",
    "housing_train_df= pd.DataFrame(housing_train_df_imp, columns=housing_train_df.columns)\n",
    "housing_train_df.head()\n",
    "\n",
    "## Feature engineering\n",
    "## Feature Scaling\n",
    "Feature Scaling est une étape nécessaire voire indispensable de remise à niveau de caractéristiques de notre modèle de Machine Learning. Pourquoi ? et bien tout simplement car derrière chaque algorithme se cache des formules mathématiques. Et ces formules mathématiques n’apprécient guère les variations d’échelle de valeurs entre chaque caractéristiques. Et ça c’est tout particulièrement vrai en ce qui concerne la descente de gradient !\n",
    "\n",
    "    Si vous ne faites rien vous allez observer des lenteurs d’apprentissage et des performances amoindries.\n",
    "\n",
    "Prenons un exemple. Imaginez que vous travaillez sur une modélisation autour de données immobilières. Vous aurez des caractéristiques du type : prix, surface, nombre de pièces, etc. Bien sur les échelles de valeurs de ces données sont totalement différentes selon les caractéristiques. Néanmoins vous allez devoir les traiter via le même algorithme. Votre algorithme va en effet devoir mixer des prix de [0 … 100000]€, des surfaces de [0 … 300] m2, des nombres de pièces allant de [1 .. 10] pièces. \n",
    "\n",
    "La mise à l’échelle consiste donc à mettre au même niveaux ces données.\n",
    "\n",
    "#### StandardScaler()\n",
    "\n",
    "Cette technique part du principe que les données sont normalement distribuées. La fonction va recalculer chaque caractéristiques (Cf. formule ci-dessous) afin que les données soient centré autour de 0 et avec un Ecart-Type de 1.\n",
    "\n",
    "<img src=\"standardscaler.jpg\">\n",
    "\n",
    "mean(x) : Moyenne\n",
    "\n",
    "stdev(x) : « Standard Deviation » en Anglais signifie Ecart-Type\n",
    "\n",
    "# Check StandardScaler in Scikit Learn\n",
    "# your code\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaled_df = scaler.fit_transform(x_train)\n",
    "x_train = pd.DataFrame(scaled_df, columns=x_train.columns)\n",
    "\n",
    "x_train.head()\n",
    "\n",
    "# Select and Train a Model\n",
    "## Training and Evaluating on the Training Set\n",
    "Try :\n",
    "- Linear Regression\n",
    "- Decision Tree Regression\n",
    "- Random Forest Regression\n",
    "(check Scikit Learn)\n",
    "\n",
    "With Performance Metric = RMSE\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "## Model Evaluation Using Cross-Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(lin_reg, x_train, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "\n",
    "x_test, y_test\n",
    "\n",
    "- separer les features de la target (X et Y)\n",
    "- le get dummies => dans une autre DF\n",
    "- le cleaning imp.transform()\n",
    "- ajout des colonnes\n",
    "- le scaler\n",
    "\n",
    "Une bonne pratique tout mettre dans une fonction. => on peut faire plusieurs test avec différents bout de datatset\n",
    "\n",
    "### Travail sur le test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
